/*******************************************************************************
 * This file contains TCM_QLA2XXX functions for struct target_core_fabrib_ops
 * for Qlogic 2xxx series target mode HBAs
 *
 * Â© Copyright 2010-2011 RisingTide Systems LLC.
 *
 * Licensed to the Linux Foundation under the General Public License (GPL) version 2.
 *
 * Author: Nicholas A. Bellinger <nab@risingtidesystems.com>
 *
 * tcm_qla2xxx_parse_wwn() and tcm_qla2xxx_format_wwn() contains code from
 * the TCM_FC / Open-FCoE.org fabric module.
 *
 * Copyright (c) 2010 Cisco Systems, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 ****************************************************************************/

#include <linux/slab.h>
#include <linux/kthread.h>
#include <linux/types.h>
#include <linux/list.h>
#include <linux/types.h>
#include <linux/string.h>
#include <linux/ctype.h>
#include <asm/unaligned.h>
#include <scsi/scsi.h>
#include <scsi/scsi_host.h>
#include <scsi/scsi_device.h>
#include <scsi/scsi_cmnd.h>

#include <target/target_core_base.h>
#include <target/target_core_transport.h>
#include <target/target_core_fabric_ops.h>
#include <target/target_core_fabric_lib.h>
#include <target/target_core_device.h>
#include <target/target_core_tpg.h>
#include <target/target_core_configfs.h>
#include <target/target_core_tmr.h>

#include <qla_def.h>
#include <qla_target.h>

#include "tcm_qla2xxx_base.h"
#include "tcm_qla2xxx_fabric.h"

extern struct workqueue_struct *tcm_qla2xxx_free_wq;
extern struct workqueue_struct *tcm_qla2xxx_cmd_wq;

int tcm_qla2xxx_check_true(struct se_portal_group *se_tpg)
{
	return 1;
}

int tcm_qla2xxx_check_false(struct se_portal_group *se_tpg)
{
	return 0;
}

/*
 * Parse WWN.
 * If strict, we require lower-case hex and colon separators to be sure
 * the name is the same as what would be generated by ft_format_wwn()
 * so the name and wwn are mapped one-to-one.
 */
ssize_t tcm_qla2xxx_parse_wwn(const char *name, u64 *wwn, int strict)
{
	const char *cp;
	char c;
	u32 nibble;
	u32 byte = 0;
	u32 pos = 0;
	u32 err;

	*wwn = 0;
	for (cp = name; cp < &name[TCM_QLA2XXX_NAMELEN - 1]; cp++) {
		c = *cp;
		if (c == '\n' && cp[1] == '\0')
			continue;
		if (strict && pos++ == 2 && byte++ < 7) {
			pos = 0;
			if (c == ':')
				continue;
			err = 1;
			goto fail;
		}
		if (c == '\0') {
			err = 2;
			if (strict && byte != 8)
				goto fail;
			return cp - name;
		}
		err = 3;
		if (isdigit(c))
			nibble = c - '0';
		else if (isxdigit(c) && (islower(c) || !strict))
			nibble = tolower(c) - 'a' + 10;
		else
			goto fail;
		*wwn = (*wwn << 4) | nibble;
	}
	err = 4;
fail:
	pr_debug("err %u len %zu pos %u byte %u\n",
			err, cp - name, pos, byte);
	return -1;
}

ssize_t tcm_qla2xxx_format_wwn(char *buf, size_t len, u64 wwn)
{
	u8 b[8];

	put_unaligned_be64(wwn, b);
	return snprintf(buf, len,
		"%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x",
		b[0], b[1], b[2], b[3], b[4], b[5], b[6], b[7]);
}

char *tcm_qla2xxx_get_fabric_name(void)
{
	return "qla2xxx";
}

/*
 * From drivers/scsi/scsi_transport_fc.c:fc_parse_wwn
 */
static int tcm_qla2xxx_npiv_extract_wwn(const char *ns, u64 *nm)
{
	unsigned int i, j, value;
	u8 wwn[8];

	memset(wwn, 0, sizeof(wwn));

	/* Validate and store the new name */
	for (i = 0, j = 0; i < 16; i++) {
		value = hex_to_bin(*ns++);
		if (value >= 0)
			j = (j << 4) | value;
		else
			return -EINVAL;

		if (i % 2) {
			wwn[i/2] = j & 0xff;
			j = 0;
		}
	}

	*nm = wwn_to_u64(wwn);
	return 0;
}

/*
 * This parsing logic follows drivers/scsi/scsi_transport_fc.c:store_fc_host_vport_create()
 */
int tcm_qla2xxx_npiv_parse_wwn(
	const char *name,
	size_t count,
	u64 *wwpn,
	u64 *wwnn)
{
	unsigned int cnt = count;
	int rc;

	*wwpn = 0;
	*wwnn = 0;

	/* count may include a LF at end of string */
	if (name[cnt-1] == '\n')
		cnt--;

	/* validate we have enough characters for WWPN */
	if ((cnt != (16+1+16)) || (name[16] != ':'))
		return -EINVAL;

	rc = tcm_qla2xxx_npiv_extract_wwn(&name[0], wwpn);
	if (rc != 0)
		return rc;

	rc = tcm_qla2xxx_npiv_extract_wwn(&name[17], wwnn);
	if (rc != 0)
		return rc;

	return 0;
}

ssize_t tcm_qla2xxx_npiv_format_wwn(char *buf, size_t len, u64 wwpn, u64 wwnn)
{
	u8 b[8], b2[8];

	put_unaligned_be64(wwpn, b);
	put_unaligned_be64(wwnn, b2);
        return snprintf(buf, len,
                "%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x,"
		"%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x",
                b[0], b[1], b[2], b[3], b[4], b[5], b[6], b[7],
		b2[0], b2[1], b2[2], b2[3], b2[4], b2[5], b2[6], b2[7]);
}

char *tcm_qla2xxx_npiv_get_fabric_name(void)
{
	return "qla2xxx_npiv";
}

u8 tcm_qla2xxx_get_fabric_proto_ident(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	u8 proto_id;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		proto_id = fc_get_fabric_proto_ident(se_tpg);
		break;
	}

	return proto_id;
}

char *tcm_qla2xxx_get_fabric_wwn(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;

	return &lport->lport_name[0];
}

char *tcm_qla2xxx_npiv_get_fabric_wwn(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;

	return &lport->lport_npiv_name[0];
}

u16 tcm_qla2xxx_get_tag(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	return tpg->lport_tpgt;
}

u32 tcm_qla2xxx_get_default_depth(struct se_portal_group *se_tpg)
{
	return 1;
}

u32 tcm_qla2xxx_get_pr_transport_id(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl,
	struct t10_pr_registration *pr_reg,
	int *format_code,
	unsigned char *buf)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	int ret = 0;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		ret = fc_get_pr_transport_id(se_tpg, se_nacl, pr_reg,
					format_code, buf);
		break;
	}

	return ret;
}		

u32 tcm_qla2xxx_get_pr_transport_id_len(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl,
	struct t10_pr_registration *pr_reg,
	int *format_code)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	int ret = 0;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		ret = fc_get_pr_transport_id_len(se_tpg, se_nacl, pr_reg,
					format_code);
		break;
	}

	return ret;
}

char *tcm_qla2xxx_parse_pr_out_transport_id(
	struct se_portal_group *se_tpg,
	const char *buf,
	u32 *out_tid_len,
	char **port_nexus_ptr)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	char *tid = NULL;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		tid = fc_parse_pr_out_transport_id(se_tpg, buf, out_tid_len,
					port_nexus_ptr);
		break;
	}

	return tid;
}

int tcm_qla2xxx_check_demo_mode(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return QLA_TPG_ATTRIB(tpg)->generate_node_acls;
}

int tcm_qla2xxx_check_demo_mode_cache(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return QLA_TPG_ATTRIB(tpg)->cache_dynamic_acls;
}

int tcm_qla2xxx_check_demo_write_protect(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return QLA_TPG_ATTRIB(tpg)->demo_mode_write_protect;
}

int tcm_qla2xxx_check_prod_write_protect(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return QLA_TPG_ATTRIB(tpg)->prod_mode_write_protect;
}

struct se_node_acl *tcm_qla2xxx_alloc_fabric_acl(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_nacl *nacl;

	nacl = kzalloc(sizeof(struct tcm_qla2xxx_nacl), GFP_KERNEL);
	if (!nacl) {
		pr_err("Unable to alocate struct tcm_qla2xxx_nacl\n");
		return NULL;
	}

	return &nacl->se_node_acl;
}

void tcm_qla2xxx_release_fabric_acl(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl)
{
	struct tcm_qla2xxx_nacl *nacl = container_of(se_nacl,
			struct tcm_qla2xxx_nacl, se_node_acl);
	kfree(nacl);
}

u32 tcm_qla2xxx_tpg_get_inst_index(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return tpg->lport_tpgt;
}

static void tcm_qla2xxx_complete_free(struct work_struct *work)
{
	struct qla_tgt_cmd *cmd = container_of(work, struct qla_tgt_cmd, work);

	transport_generic_free_cmd(&cmd->se_cmd, 0);
}

/*
 * Called from qla_target_template->free_cmd(), and will call
 * tcm_qla2xxx_release_cmd via normal struct target_core_fabric_ops
 * release callback.  qla_hw_data->hardware_lock is expected to be held
 */
void tcm_qla2xxx_free_cmd(struct qla_tgt_cmd *cmd)
{
	barrier();
	/*
	 * Handle tcm_qla2xxx_handle_cmd() -> transport_get_lun_for_cmd()
	 * failure case where cmd->se_cmd.se_dev was not assigned, and
	 * a call to transport_generic_free_cmd_intr() is not possible..
	 */
	if (!cmd->se_cmd.se_dev) {
		atomic_set(&cmd->cmd_stop_free, 1);
		atomic_set(&cmd->cmd_free, 1);
		smp_mb__after_atomic_dec();
		transport_generic_free_cmd(&cmd->se_cmd, 0);
		return;
	}

	if (!atomic_read(&cmd->se_cmd.t_transport_complete)) {
		atomic_set(&cmd->cmd_stop_free, 1);
		smp_mb__after_atomic_dec();
	}

	atomic_set(&cmd->cmd_free, 1);
	smp_mb__after_atomic_dec();

	INIT_WORK(&cmd->work, tcm_qla2xxx_complete_free);
	queue_work(tcm_qla2xxx_free_wq, &cmd->work);
}

/*
 * Called from struct target_core_fabric_ops->check_stop_free() context
 */
int tcm_qla2xxx_check_stop_free(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	struct qla_tgt_mgmt_cmd *mcmd;
	struct qla_hw_data *ha;
	unsigned long flags;

	if (se_cmd->se_tmr_req) {
		mcmd = container_of(se_cmd, struct qla_tgt_mgmt_cmd, se_cmd);
		/*
		 * Release the associated se_cmd->se_tmr_req and se_cmd
		 * TMR related state now.
		 */
		transport_generic_free_cmd(se_cmd, 1);
		qla_tgt_free_mcmd(mcmd);
		return 1;
	}
	ha = cmd->sess->vha->hw;
	/*
 	 * Set cmd_stop_free and wakeup cmd_stop_free_comp if necessary
 	 * if tcm_qla2xxx_release_cmd() context is waiting for completion.
 	 */
	spin_lock_irqsave(&ha->hardware_lock, flags);
	atomic_set(&cmd->cmd_stop_free, 1);
	if (atomic_read(&cmd->cmd_free) != 0)
		complete(&cmd->cmd_stop_free_comp);
	spin_unlock_irqrestore(&ha->hardware_lock, flags);

	return 0;
}

int tcm_qla2xxx_check_release_cmd(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd;
	struct qla_tgt_sess *sess;
	struct qla_hw_data *ha;
	unsigned long flags;
	int ret = 0;

	if (se_cmd->se_tmr_req != NULL)
		return 0;

	cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	sess = cmd->sess;

	if (!sess)
		BUG();

	ha = sess->vha->hw;
	/*
	 * If the callback to tcm_qla2xxx_check_stop_free() has not finished,
	 * before the release path is invoked, go ahead and wait on
	 * cmd_stop_free_comp until tcm_qla2xxx_check_stop_free completes.
	 */
	spin_lock_irqsave(&ha->hardware_lock, flags);
	if (atomic_read(&cmd->cmd_stop_free) != 1) {
		pr_warn("Detected cmd->cmd_stop_free != 0, waiting on"
			" cmd_stop_free_comp for cmd: %p\n", cmd);
		spin_unlock_irqrestore(&ha->hardware_lock, flags);
		wait_for_completion(&cmd->cmd_stop_free_comp);
		spin_lock_irqsave(&ha->hardware_lock, flags);
	}
	ret = target_put_sess_cmd(sess->se_sess, se_cmd);
	spin_unlock_irqrestore(&ha->hardware_lock, flags);

	return ret;
}

/* tcm_qla2xxx_release_cmd - Callback from TCM Core to release underlying fabric descriptor
 * @se_cmd command to release
 */
void tcm_qla2xxx_release_cmd(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd;

	if (se_cmd->se_tmr_req != NULL)
		return;

	cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	qla_tgt_free_cmd(cmd);
}

int tcm_qla2xxx_shutdown_session(struct se_session *se_sess)
{
	struct qla_tgt_sess *sess = se_sess->fabric_sess_ptr;

	if (!sess) {
		pr_err("se_sess->fabric_sess_ptr is NULL\n");
		dump_stack();
		return 0;
	}
	return 1;
}

extern int tcm_qla2xxx_clear_nacl_from_fcport_map(struct se_node_acl *);

void tcm_qla2xxx_close_session(struct se_session *se_sess)
{
	struct se_node_acl *se_nacl = se_sess->se_node_acl;
	struct qla_tgt_sess *sess = se_sess->fabric_sess_ptr;
	struct scsi_qla_host *vha;
	unsigned long flags;

	if (!sess) {
		pr_err("se_sess->fabric_sess_ptr is NULL\n");
		dump_stack();
		return;
	}
	vha = sess->vha;

	spin_lock_irqsave(&vha->hw->hardware_lock, flags);
	tcm_qla2xxx_clear_nacl_from_fcport_map(se_nacl);
	qla_tgt_sess_put(sess);
	spin_unlock_irqrestore(&vha->hw->hardware_lock, flags);
}

void tcm_qla2xxx_stop_session(struct se_session *se_sess, int sess_sleep , int conn_sleep)
{
	struct qla_tgt_sess *sess = se_sess->fabric_sess_ptr;
	struct scsi_qla_host *vha;
	unsigned long flags;

	if (!sess) {
		pr_err("se_sess->fabric_sess_ptr is NULL\n");
		dump_stack();
		return;
	}
	vha = sess->vha;

	spin_lock_irqsave(&vha->hw->hardware_lock, flags);
	tcm_qla2xxx_clear_nacl_from_fcport_map(se_sess->se_node_acl);
	spin_unlock_irqrestore(&vha->hw->hardware_lock, flags);
}

void tcm_qla2xxx_reset_nexus(struct se_session *se_sess)
{
	return;
}

int tcm_qla2xxx_sess_logged_in(struct se_session *se_sess)
{
	return 0;
}

u32 tcm_qla2xxx_sess_get_index(struct se_session *se_sess)
{
	return 0;
}

/*
 * The LIO target core uses DMA_TO_DEVICE to mean that data is going
 * to the target (eg handling a WRITE) and DMA_FROM_DEVICE to mean
 * that data is coming from the target (eg handling a READ).  However,
 * this is just the opposite of what we have to tell the DMA mapping
 * layer -- eg when handling a READ, the HBA will have to DMA the data
 * out of memory so it can send it to the initiator, which means we
 * need to use DMA_TO_DEVICE when we map the data.
 */
static enum dma_data_direction tcm_qla2xxx_mapping_dir(struct se_cmd *se_cmd)
{
	if (se_cmd->t_tasks_bidi)
		return DMA_BIDIRECTIONAL;

	switch (se_cmd->data_direction) {
	case DMA_TO_DEVICE:
		return DMA_FROM_DEVICE;
	case DMA_FROM_DEVICE:
		return DMA_TO_DEVICE;
	case DMA_NONE:
	default:
		return DMA_NONE;
	}
}

int tcm_qla2xxx_write_pending(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	cmd->bufflen = se_cmd->data_length;
	cmd->dma_data_direction = tcm_qla2xxx_mapping_dir(se_cmd);

	/*
	 * Setup the struct se_task->task_sg[] chained SG list
	 */
	transport_do_task_sg_chain(se_cmd);
	cmd->sg_cnt = se_cmd->t_tasks_sg_chained_no;
	cmd->sg = se_cmd->t_tasks_sg_chained;

	/*
	 * qla_target.c:qla_tgt_rdy_to_xfer() will call pci_map_sg() to setup
	 * the SGL mappings into PCIe memory for incoming FCP WRITE data.
	 */
	return qla_tgt_rdy_to_xfer(cmd);
}

int tcm_qla2xxx_write_pending_status(struct se_cmd *se_cmd)
{
	unsigned long flags;
	/*
	 * Check for WRITE_PENDING status to determine if we need to wait for
	 * CTIO aborts to be posted via hardware in tcm_qla2xxx_handle_data().
	 */
	spin_lock_irqsave(&se_cmd->t_state_lock, flags);
	if (se_cmd->t_state == TRANSPORT_WRITE_PENDING ||
	    se_cmd->t_state == TRANSPORT_COMPLETE_QF_WP) {
		spin_unlock_irqrestore(&se_cmd->t_state_lock, flags);
		wait_for_completion_timeout(&se_cmd->t_transport_stop_comp, 3000);
		return 0;
	}
	spin_unlock_irqrestore(&se_cmd->t_state_lock, flags);

	return 0;
}

void tcm_qla2xxx_set_default_node_attrs(struct se_node_acl *nacl)
{
	return;
}

u32 tcm_qla2xxx_get_task_tag(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	return cmd->tag;
}

int tcm_qla2xxx_get_cmd_state(struct se_cmd *se_cmd)
{
	return 0;
}

static void tcm_qla2xxx_do_work(struct work_struct *);

/*
 * Main entry point for incoming ATIO packets from qla_target.c
 * and qla2xxx LLD code.  Called with qla_hw_data->hardware_lock held
 */
int tcm_qla2xxx_handle_cmd(scsi_qla_host_t *vha, struct qla_tgt_cmd *cmd,
			uint32_t data_length, int fcp_task_attr,
			int data_dir, int bidi)
{
	struct se_cmd *se_cmd = &cmd->se_cmd;
	struct se_session *se_sess;
	struct se_portal_group *se_tpg;
	struct qla_tgt_sess *sess;

	sess = cmd->sess;
	if (!sess) {
		pr_err("Unable to locate struct qla_tgt_sess from qla_tgt_cmd\n");
		return -EINVAL;
	}

	se_sess = sess->se_sess;
	if (!se_sess) {
		pr_err("Unable to locate active struct se_session\n");
		return -EINVAL;
	}
	se_tpg = se_sess->se_tpg;

	/*
	 * Initialize struct se_cmd descriptor from target_core_mod infrastructure
	 */
	transport_init_se_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess,
			data_length, data_dir,
			fcp_task_attr, &cmd->sense_buffer[0]);
	/*
 	 * Protected by qla_hw_data->hardware_lock
 	 */
	target_get_sess_cmd(se_sess, se_cmd);
	/*
	 * Signal BIDI usage with T_TASK(cmd)->t_tasks_bidi
	 */
	if (bidi)
		se_cmd->t_tasks_bidi = 1;

	INIT_WORK(&cmd->work, tcm_qla2xxx_do_work);
	queue_work(cmd->tgt->qla_tgt_wq, &cmd->work);
	return 0;
}

void tcm_qla2xxx_do_work(struct work_struct *work)
{
	struct qla_tgt_cmd *cmd = container_of(work, struct qla_tgt_cmd, work);
	struct se_cmd *se_cmd = &cmd->se_cmd;
	scsi_qla_host_t *vha = cmd->vha;
	struct qla_hw_data *ha = vha->hw;
	unsigned char *cdb;
	atio_from_isp_t *atio = &cmd->atio;
	int rc;

	if (IS_FWI2_CAPABLE(ha))
		cdb = &atio->u.isp24.fcp_cmnd.cdb[0];
	else
		cdb = &atio->u.isp2x.cdb[0];
	/*
 	 * Locate se_lun pointer and attach it to struct se_cmd
 	 */
	if (transport_lookup_cmd_lun(se_cmd, cmd->unpacked_lun) < 0) {
		transport_send_check_condition_and_sense(se_cmd,
			se_cmd->scsi_sense_reason, 0);
		return;
	}

	/*
	 * Allocate the necessary tasks to complete the received CDB+data
	 * drivers/target/target_core_transport.c:transport_processing_thread()
	 * falls through to TRANSPORT_NEW_CMD.
	 */
	rc = transport_generic_allocate_tasks(se_cmd, cdb);
	if (rc != 0) {
		transport_send_check_condition_and_sense(se_cmd,
				se_cmd->scsi_sense_reason, 0);
		return;
	}
	transport_handle_cdb_direct(se_cmd);
}

void tcm_qla2xxx_do_rsp(struct work_struct *work)
{
	struct qla_tgt_cmd *cmd = container_of(work, struct qla_tgt_cmd, work);
	/*
	 * Dispatch ->queue_status from workqueue process context
	 */
	transport_send_check_condition_and_sense(&cmd->se_cmd,
				cmd->se_cmd.scsi_sense_reason, 0);
}

/*
 * Called from qla_target.c:qla_tgt_do_ctio_completion()
 */
int tcm_qla2xxx_handle_data(struct qla_tgt_cmd *cmd)
{
	struct se_cmd *se_cmd = &cmd->se_cmd;
	unsigned long flags;
	/*
	 * Ensure that the complete FCP WRITE payload has been received.
	 * Otherwise return an exception via CHECK_CONDITION status.
	 */
	if (!cmd->write_data_transferred) {
		/*
		 * Check if se_cmd has already been aborted via LUN_RESET, and is
		 * waiting upon completion in tcm_qla2xxx_write_pending_status()..
		 */
		spin_lock_irqsave(&se_cmd->t_state_lock, flags);
		if (atomic_read(&se_cmd->t_transport_aborted)) {
			spin_unlock_irqrestore(&se_cmd->t_state_lock, flags);
			complete(&se_cmd->t_transport_stop_comp);
			return 0;
		}
		spin_unlock_irqrestore(&se_cmd->t_state_lock, flags);

		se_cmd->scsi_sense_reason = TCM_CHECK_CONDITION_ABORT_CMD;
		INIT_WORK(&cmd->work, tcm_qla2xxx_do_rsp);
		queue_work(cmd->tgt->qla_tgt_wq, &cmd->work);
		return 0;
	}
	/*
	 * We now tell TCM to queue this WRITE CDB with TRANSPORT_PROCESS_WRITE
	 * status to the backstore processing thread.
	 */
	return transport_generic_handle_data(&cmd->se_cmd);
}

/*
 * Called from qla_target.c:qla_tgt_issue_task_mgmt()
 */
int tcm_qla2xxx_handle_tmr(struct qla_tgt_mgmt_cmd *mcmd, uint32_t lun, uint8_t tmr_func)
{
	struct qla_tgt_sess *sess = mcmd->sess;
	struct se_session *se_sess = sess->se_sess;
	struct se_portal_group *se_tpg = se_sess->se_tpg;
	struct se_cmd *se_cmd = &mcmd->se_cmd;
	/*
	 * Initialize struct se_cmd descriptor from target_core_mod infrastructure
	 */
	transport_init_se_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess, 0,
				DMA_NONE, 0, NULL);
	/*
	 * Allocate the TCM TMR
	 */
	se_cmd->se_tmr_req = core_tmr_alloc_req(se_cmd, mcmd, tmr_func, GFP_ATOMIC);
	if (!se_cmd->se_tmr_req)
		return -ENOMEM;
	/*
	 * Save the se_tmr_req for qla_tgt_xmit_tm_rsp() callback into LLD code
	 */
	mcmd->se_tmr_req = se_cmd->se_tmr_req;
	/*
	 * Locate the underlying TCM struct se_lun from sc->device->lun
	 */
	if (transport_lookup_tmr_lun(se_cmd, lun) < 0) {
		transport_generic_free_cmd(se_cmd, 1);
		return -EINVAL;
	}
	/*
	 * Queue the TMR associated se_cmd into TCM Core for processing
	 */
	return transport_generic_handle_tmr(se_cmd);
}

int tcm_qla2xxx_queue_data_in(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	cmd->bufflen = se_cmd->data_length;
	cmd->dma_data_direction = tcm_qla2xxx_mapping_dir(se_cmd);
	cmd->aborted = atomic_read(&se_cmd->t_transport_aborted);

	/*
	 * Setup the struct se_task->task_sg[] chained SG list
	 */
	transport_do_task_sg_chain(se_cmd);
	cmd->sg_cnt = se_cmd->t_tasks_sg_chained_no;
	cmd->sg = se_cmd->t_tasks_sg_chained;
	cmd->offset = 0;

	/*
	 * Now queue completed DATA_IN the qla2xxx LLD and response ring
	 */
	return qla_tgt_xmit_response(cmd, QLA_TGT_XMIT_DATA|QLA_TGT_XMIT_STATUS,
				se_cmd->scsi_status);
}

int tcm_qla2xxx_queue_status(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	int xmit_type = QLA_TGT_XMIT_STATUS;

	cmd->bufflen = se_cmd->data_length;
	cmd->sg = NULL;
	cmd->sg_cnt = 0;
	cmd->offset = 0;
	cmd->dma_data_direction = tcm_qla2xxx_mapping_dir(se_cmd);
	cmd->aborted = atomic_read(&se_cmd->t_transport_aborted);

	if (se_cmd->data_direction == DMA_FROM_DEVICE) {
		/*
		 * For FCP_READ with CHECK_CONDITION status, clear cmd->bufflen
		 * for qla_tgt_xmit_response LLD code
		 */
		se_cmd->se_cmd_flags |= SCF_UNDERFLOW_BIT;
		se_cmd->residual_count = se_cmd->data_length;

		cmd->bufflen = 0;
	}
	/*
	 * Now queue status response to qla2xxx LLD code and response ring
	 */
	return qla_tgt_xmit_response(cmd, xmit_type, se_cmd->scsi_status);
}

int tcm_qla2xxx_queue_tm_rsp(struct se_cmd *se_cmd)
{
	struct se_tmr_req *se_tmr = se_cmd->se_tmr_req;
	struct qla_tgt_mgmt_cmd *mcmd = container_of(se_cmd,
				struct qla_tgt_mgmt_cmd, se_cmd);

	pr_debug("queue_tm_rsp: mcmd: %p func: 0x%02x response: 0x%02x\n",
			mcmd, se_tmr->function, se_tmr->response);
	/*
	 * Do translation between TCM TM response codes and
	 * QLA2xxx FC TM response codes.
	 */
	switch (se_tmr->response) {
	case TMR_FUNCTION_COMPLETE:
		mcmd->fc_tm_rsp = FC_TM_SUCCESS;
		break;
	case TMR_TASK_DOES_NOT_EXIST:
		mcmd->fc_tm_rsp = FC_TM_BAD_CMD;
		break;
	case TMR_FUNCTION_REJECTED:
		mcmd->fc_tm_rsp = FC_TM_REJECT;
		break;
	case TMR_LUN_DOES_NOT_EXIST:
	default:
		mcmd->fc_tm_rsp = FC_TM_FAILED;
		break;
	}
	/*
	 * Queue the TM response to QLA2xxx LLD to build a
	 * CTIO response packet.
	 */
	qla_tgt_xmit_tm_rsp(mcmd);

	return 0;
}

u16 tcm_qla2xxx_get_fabric_sense_len(void)
{
	return 0;
}

u16 tcm_qla2xxx_set_fabric_sense_len(struct se_cmd *se_cmd, u32 sense_length)
{
	return 0;
}

int tcm_qla2xxx_is_state_remove(struct se_cmd *se_cmd)
{
	return 0;
}
