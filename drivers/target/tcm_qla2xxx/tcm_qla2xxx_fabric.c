/*******************************************************************************
 * Filename:  tcm_qla2xxx_fabric.c
 *
 * This file contains TCM_QLA2XXX functions for struct target_core_fabrib_ops
 * for Qlogic 2xxx series target mode HBAs
 *
 * Copyright (c) 2010 Rising Tide Systems, Inc
 * Copyright (c) 2010 Linux-iSCSI.org
 *
 * Copyright (c) 2010 Nicholas A. Bellinger <nab@linux-iscsi.org>
 *
 * tcm_qla2xxx_parse_wwn() and tcm_qla2xxx_format_wwn() contains code from
 * the TCM_FC / Open-FCoE.org fabric module.
 *
 * Copyright (c) 2010 Cisco Systems, Inc
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 ****************************************************************************/

#define TCM_QLA2XXX_FABRIC_C

#include <linux/slab.h>
#include <linux/kthread.h>
#include <linux/types.h>
#include <linux/list.h>
#include <linux/types.h>
#include <linux/string.h>
#include <linux/ctype.h>
#include <asm/unaligned.h>
#include <scsi/scsi.h>
#include <scsi/scsi_host.h>
#include <scsi/scsi_device.h>
#include <scsi/scsi_cmnd.h>

#include <target/target_core_base.h>
#include <target/target_core_transport.h>
#include <target/target_core_fabric_ops.h>
#include <target/target_core_fabric_lib.h>
#include <target/target_core_device.h>
#include <target/target_core_tpg.h>
#include <target/target_core_configfs.h>
#include <target/target_core_tmr.h>

#include <qla_def.h>

//#include <scsi/libfc.h>
//#include <scsi/scsi_transport_fc.h>

#include <qla_target.h>

#include <tcm_qla2xxx_base.h>
#include <tcm_qla2xxx_fabric.h>

#undef TCM_QLA2XXX_FABRIC_C

int tcm_qla2xxx_check_true(struct se_portal_group *se_tpg)
{
	return 1;
}

int tcm_qla2xxx_check_false(struct se_portal_group *se_tpg)
{
	return 0;
}

/*
 * Parse WWN.
 * If strict, we require lower-case hex and colon separators to be sure
 * the name is the same as what would be generated by ft_format_wwn()
 * so the name and wwn are mapped one-to-one.
 */
ssize_t tcm_qla2xxx_parse_wwn(const char *name, u64 *wwn, int strict)
{
	const char *cp;
	char c;
	u32 nibble;
	u32 byte = 0;
	u32 pos = 0;
	u32 err;

	*wwn = 0;
	for (cp = name; cp < &name[TCM_QLA2XXX_NAMELEN - 1]; cp++) {
		c = *cp;
		if (c == '\n' && cp[1] == '\0')
			continue;
		if (strict && pos++ == 2 && byte++ < 7) {
			pos = 0;
			if (c == ':')
				continue;
			err = 1;
			goto fail;
		}
		if (c == '\0') {
			err = 2;
			if (strict && byte != 8)
				goto fail;
			return cp - name;
		}
		err = 3;
		if (isdigit(c))
			nibble = c - '0';
		else if (isxdigit(c) && (islower(c) || !strict))
			nibble = tolower(c) - 'a' + 10;
		else
			goto fail;
		*wwn = (*wwn << 4) | nibble;
	}
	err = 4;
fail:
	printk(KERN_INFO "err %u len %zu pos %u byte %u\n",
			err, cp - name, pos, byte);
	return -1;
}

ssize_t tcm_qla2xxx_format_wwn(char *buf, size_t len, u64 wwn)
{
	u8 b[8];

	put_unaligned_be64(wwn, b);
	return snprintf(buf, len,
		"%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x",
		b[0], b[1], b[2], b[3], b[4], b[5], b[6], b[7]);
}

char *tcm_qla2xxx_get_fabric_name(void)
{
	return "qla2xxx";
}

/*
 * From drivers/scsi/scsi_transport_fc.c:fc_parse_wwn
 */
static int tcm_qla2xxx_npiv_extract_wwn(const char *ns, u64 *nm)
{
	unsigned int i, j, value;
	u8 wwn[8];

	memset(wwn, 0, sizeof(wwn));

	/* Validate and store the new name */
	for (i = 0, j = 0; i < 16; i++) {
		value = hex_to_bin(*ns++);
		if (value >= 0)
			j = (j << 4) | value;
		else
			return -EINVAL;

		if (i % 2) {
			wwn[i/2] = j & 0xff;
			j = 0;
		}
	}

	*nm = wwn_to_u64(wwn);
	return 0;
}

/*
 * This parsing logic follows drivers/scsi/scsi_transport_fc.c:store_fc_host_vport_create()
 */
int tcm_qla2xxx_npiv_parse_wwn(
	const char *name,
	size_t count,
	u64 *wwpn,
	u64 *wwnn)
{
	unsigned int cnt = count;
	int rc;

	*wwpn = 0;
	*wwnn = 0;

	/* count may include a LF at end of string */
	if (name[cnt-1] == '\n')
		cnt--;

	/* validate we have enough characters for WWPN */
	if ((cnt != (16+1+16)) || (name[16] != ':'))
		return -EINVAL;

	rc = tcm_qla2xxx_npiv_extract_wwn(&name[0], wwpn);
	if (rc != 0)
		return rc;

	rc = tcm_qla2xxx_npiv_extract_wwn(&name[17], wwnn);
	if (rc != 0)
		return rc;

	return 0;
}

ssize_t tcm_qla2xxx_npiv_format_wwn(char *buf, size_t len, u64 wwpn, u64 wwnn)
{
	u8 b[8], b2[8];

	put_unaligned_be64(wwpn, b);
	put_unaligned_be64(wwnn, b2);
        return snprintf(buf, len,
                "%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x,"
		"%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x:%2.2x",
                b[0], b[1], b[2], b[3], b[4], b[5], b[6], b[7],
		b2[0], b2[1], b2[2], b2[3], b2[4], b2[5], b2[6], b2[7]);
}

char *tcm_qla2xxx_npiv_get_fabric_name(void)
{
	return "qla2xxx_npiv";
}

u8 tcm_qla2xxx_get_fabric_proto_ident(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	u8 proto_id;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		proto_id = fc_get_fabric_proto_ident(se_tpg);
		break;
	}

	return proto_id;
}

char *tcm_qla2xxx_get_fabric_wwn(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;

	return &lport->lport_name[0];
}

char *tcm_qla2xxx_npiv_get_fabric_wwn(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;

	return &lport->lport_npiv_name[0];
}

u16 tcm_qla2xxx_get_tag(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	return tpg->lport_tpgt;
}

u32 tcm_qla2xxx_get_default_depth(struct se_portal_group *se_tpg)
{
	return 1;
}

u32 tcm_qla2xxx_get_pr_transport_id(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl,
	struct t10_pr_registration *pr_reg,
	int *format_code,
	unsigned char *buf)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	int ret = 0;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		ret = fc_get_pr_transport_id(se_tpg, se_nacl, pr_reg,
					format_code, buf);
		break;
	}

	return ret;
}		

u32 tcm_qla2xxx_get_pr_transport_id_len(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl,
	struct t10_pr_registration *pr_reg,
	int *format_code)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	int ret = 0;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		ret = fc_get_pr_transport_id_len(se_tpg, se_nacl, pr_reg,
					format_code);
		break;
	}

	return ret;
}

char *tcm_qla2xxx_parse_pr_out_transport_id(
	struct se_portal_group *se_tpg,
	const char *buf,
	u32 *out_tid_len,
	char **port_nexus_ptr)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);
	struct tcm_qla2xxx_lport *lport = tpg->lport;
	char *tid = NULL;

	switch (lport->lport_proto_id) {
	case SCSI_PROTOCOL_FCP:
	default:
		tid = fc_parse_pr_out_transport_id(se_tpg, buf, out_tid_len,
					port_nexus_ptr);
		break;
	}

	return tid;
}

struct se_node_acl *tcm_qla2xxx_alloc_fabric_acl(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_nacl *nacl;

	nacl = kzalloc(sizeof(struct tcm_qla2xxx_nacl), GFP_KERNEL);
	if (!(nacl)) {
		printk(KERN_ERR "Unable to alocate struct tcm_qla2xxx_nacl\n");
		return NULL;
	}

	return &nacl->se_node_acl;
}

void tcm_qla2xxx_release_fabric_acl(
	struct se_portal_group *se_tpg,
	struct se_node_acl *se_nacl)
{
	struct tcm_qla2xxx_nacl *nacl = container_of(se_nacl,
			struct tcm_qla2xxx_nacl, se_node_acl);
	kfree(nacl);
}

u32 tcm_qla2xxx_tpg_get_inst_index(struct se_portal_group *se_tpg)
{
	struct tcm_qla2xxx_tpg *tpg = container_of(se_tpg,
				struct tcm_qla2xxx_tpg, se_tpg);

	return tpg->lport_tpgt;
}

/*
 * Called from qla_target_template->free_cmd(), and will call
 * tcm_qla2xxx_release_cmd via normal struct target_core_fabric_ops
 * release callback.
 */
void tcm_qla2xxx_free_cmd(struct qla_tgt_cmd *cmd)
{
	atomic_set(&cmd->cmd_done, 1);
	/*
	 * If tcm_qla2xxx_check_stop_free() has already been called, we
	 * are safe to go ahead and call transport_generic_free_cmd()
	 * to release the descriptor.
	 */
	if (atomic_read(&cmd->cmd_stop_free) != 0)
		transport_generic_free_cmd(&cmd->se_cmd, 1, 1, 0);
}

/*
 * Called from struct target_core_fabric_ops->check_stop_free() context
 */
void tcm_qla2xxx_check_stop_free(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd;
	struct qla_tgt_mgmt_cmd *mcmd;

	if (se_cmd->se_tmr_req) {
		mcmd = container_of(se_cmd, struct qla_tgt_mgmt_cmd, se_cmd);
		/*
		 * Release the associated se_cmd->se_tmr_req and se_cmd
		 * TMR related state now.
		 */
		transport_generic_free_cmd(se_cmd, 1, 1, 0);
		qla_tgt_free_mcmd(mcmd);
		return;
	}

	cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	/*
	 * If tcm_qla2xxx_free_cmd() has already been called from the LLD,
	 * it's safe to call transport_generic_free_cmd() to release the
	 * descriptor.  Otherwise set cmd->cmd_stop_free=1 and let
	 * tcm_qla2xxx_free_cmd() call transport_generic_free_cmd().
	 */
	if (atomic_read(&cmd->cmd_done) != 0)
		transport_generic_free_cmd(se_cmd, 0, 1, 0);
	else
		atomic_set(&cmd->cmd_stop_free, 1);
}

/*
 * Callback from TCM Core to release underlying fabric descriptor
 */
void tcm_qla2xxx_release_cmd(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	if (se_cmd->se_tmr_req != NULL)
		return;

	qla_tgt_free_cmd(cmd);
}

#warning FIXME: tcm_qla2xxx_shutdown_session
int tcm_qla2xxx_shutdown_session(struct se_session *se_sess)
{
	printk("tcm_qla2xxx_shutdown_session returning TRUE\n");
	return 1;
}

extern int tcm_qla2xxx_clear_nacl_from_fcport_map(struct se_node_acl *);

void tcm_qla2xxx_close_session(struct se_session *se_sess)
{
	tcm_qla2xxx_clear_nacl_from_fcport_map(se_sess->se_node_acl);
}

void tcm_qla2xxx_stop_session(struct se_session *se_sess, int sess_sleep , int conn_sleep)
{
	tcm_qla2xxx_clear_nacl_from_fcport_map(se_sess->se_node_acl);
}

void tcm_qla2xxx_reset_nexus(struct se_session *se_sess)
{
	return;
}

int tcm_qla2xxx_sess_logged_in(struct se_session *se_sess)
{
	return 0;
}

u32 tcm_qla2xxx_sess_get_index(struct se_session *se_sess)
{
	return 0;
}

int tcm_qla2xxx_write_pending(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	cmd->bufflen = se_cmd->data_length;
	cmd->dma_data_direction = se_cmd->data_direction;
	/*
	 * Setup the struct se_task->task_sg[] chained SG list
	 */
	if ((se_cmd->se_cmd_flags & SCF_SCSI_DATA_SG_IO_CDB) ||
	    (se_cmd->se_cmd_flags & SCF_SCSI_CONTROL_SG_IO_CDB)) {
		transport_do_task_sg_chain(se_cmd);

		cmd->sg_cnt = T_TASK(se_cmd)->t_tasks_sg_chained_no;
		cmd->sg = T_TASK(se_cmd)->t_tasks_sg_chained;
	} else if (se_cmd->se_cmd_flags & SCF_SCSI_CONTROL_NONSG_IO_CDB) {
		/*
		 * Use T_TASK(se_cmd)->t_tasks_sg_bounce for control CDBs
		 * using a contigious buffer
		 */
		sg_init_table(&T_TASK(se_cmd)->t_tasks_sg_bounce, 1);
		sg_set_buf(&T_TASK(se_cmd)->t_tasks_sg_bounce,
			T_TASK(se_cmd)->t_task_buf, se_cmd->data_length);
		cmd->sg_cnt = 1;
		cmd->sg = &T_TASK(se_cmd)->t_tasks_sg_bounce;
	} else {
		printk(KERN_ERR "Unknown se_cmd_flags: 0x%08x in"
			" tcm_qla2xxx_write_pending()\n", se_cmd->se_cmd_flags);
		BUG();
	}
	/*
	 * qla_target.c:qla_tgt_rdy_to_xfer() will call pci_map_sg() to setup
	 * the SGL mappings into PCIe memory for incoming FCP WRITE data.
	 */
	return qla_tgt_rdy_to_xfer(cmd);
}

int tcm_qla2xxx_write_pending_status(struct se_cmd *se_cmd)
{
	return 0;
}

void tcm_qla2xxx_set_default_node_attrs(struct se_node_acl *nacl)
{
	return;
}

u32 tcm_qla2xxx_get_task_tag(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	return cmd->tag;
}

int tcm_qla2xxx_get_cmd_state(struct se_cmd *se_cmd)
{
	return 0;
}

void tcm_qla2xxx_new_cmd_failure(struct se_cmd *se_cmd)
{
	return;
}

/*
 * Main entry point for incoming ATIO packets from qla_target.c
 * and qla2xxx LLD code.
 */
int tcm_qla2xxx_handle_cmd(scsi_qla_host_t *vha, struct qla_tgt_cmd *cmd,
			uint32_t lun, uint32_t data_length,
			int fcp_task_attr, int data_dir, int bidi)
{
	struct se_cmd *se_cmd = &cmd->se_cmd;
	struct se_session *se_sess;
	struct se_portal_group *se_tpg;
	struct qla_tgt_sess *sess;

	sess = cmd->sess;
	if (!sess) {
		printk(KERN_ERR "Unable to locate struct qla_tgt_sess from qla_tgt_cmd\n");
		return -EINVAL;
	}

	se_sess = sess->se_sess;
	if (!se_sess) {
		printk(KERN_ERR "Unable to locate active struct se_session\n");
		return -EINVAL;
	}
	se_tpg = se_sess->se_tpg;

	/*
	 * Initialize struct se_cmd descriptor from target_core_mod infrastructure
	 */
	transport_init_se_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess,
			data_length, data_dir,
			fcp_task_attr, &cmd->sense_buffer[0]);
	/*
	 * Signal BIDI usage with T_TASK(cmd)->t_tasks_bidi
	 */
	if (bidi)
		T_TASK(se_cmd)->t_tasks_bidi = 1;
	/*
	 * Locate the struct se_lun pointer and attach it to struct se_cmd
	 */
	if (transport_get_lun_for_cmd(se_cmd, NULL, lun) < 0) {
		/*
		 * Clear qla_tgt_cmd->locked_rsp as ha->hardware_lock
		 * is already held here..
		 */
		if (spin_is_locked(&cmd->vha->hw->hardware_lock))
			cmd->locked_rsp = 0;

		/* NON_EXISTENT_LUN */
		transport_send_check_condition_and_sense(se_cmd,
				se_cmd->scsi_sense_reason, 0);
		return 0;
	}
	/*
	 * Queue up the newly allocated to be processed in TCM thread context.
	 */
	transport_device_setup_cmd(se_cmd);
	/*
	 * Queue up the newly allocated to be processed in TCM thread context.
	 */
	transport_generic_handle_cdb_map(se_cmd);
	return 0;
}

int tcm_qla2xxx_new_cmd_map(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);
	scsi_qla_host_t *vha = cmd->vha;
	struct qla_hw_data *ha = vha->hw;
	unsigned char *cdb;
	int ret;

	if (IS_FWI2_CAPABLE(ha)) {
		atio7_entry_t *atio = &cmd->atio.atio7;
		cdb = &atio->fcp_cmnd.cdb[0];
	} else {
		atio_entry_t *atio = &cmd->atio.atio2x;
		cdb = &atio->cdb[0];
	}

	/*
	 * Allocate the necessary tasks to complete the received CDB+data
	 */
	ret = transport_generic_allocate_tasks(se_cmd, cdb);
	if (ret == -1) {
		/* Out of Resources */
		transport_send_check_condition_and_sense(se_cmd,
				TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE, 0);
		return 0;
	} else if (ret == -2) {
		/*
		 * Handle case for SAM_STAT_RESERVATION_CONFLICT
		 */
		if (se_cmd->se_cmd_flags & SCF_SCSI_RESERVATION_CONFLICT) {
			tcm_qla2xxx_queue_status(se_cmd);
			return 0;
		}
		/*
		 * Otherwise, return SAM_STAT_CHECK_CONDITION and return
		 * sense data.
		 */
		transport_send_check_condition_and_sense(se_cmd,
				se_cmd->scsi_sense_reason, 0);
		return 0;
	}
	/*
	 * drivers/target/target_core_transport.c:transport_processing_thread()
	 * falls through to TRANSPORT_NEW_CMD.
	 */
	return 0;
}

/*
 * Called from qla_target.c:qla_tgt_do_ctio_completion()
 */
int tcm_qla2xxx_handle_data(struct qla_tgt_cmd *cmd)
{
	/*
	 * We now tell TCM to queue this WRITE CDB with TRANSPORT_PROCESS_WRITE
	 * status to the backstore processing thread.
	 */
	return transport_generic_handle_data(&cmd->se_cmd);
}

/*
 * Called from qla_target.c:qla_tgt_issue_task_mgmt()
 */
int tcm_qla2xxx_handle_tmr(struct qla_tgt_mgmt_cmd *mcmd, uint32_t lun, uint8_t tmr_func)
{
	struct qla_tgt_sess *sess = mcmd->sess;
	struct se_session *se_sess = sess->se_sess;
	struct se_portal_group *se_tpg = se_sess->se_tpg;
	struct se_cmd *se_cmd = &mcmd->se_cmd;
	/*
	 * Initialize struct se_cmd descriptor from target_core_mod infrastructure
	 */
	transport_init_se_cmd(se_cmd, se_tpg->se_tpg_tfo, se_sess, 0,
				DMA_NONE, 0, NULL);
	/*
	 * Allocate the TCM TMR
	 */
	se_cmd->se_tmr_req = core_tmr_alloc_req(se_cmd, (void *)mcmd, tmr_func);
	if (!se_cmd->se_tmr_req)
		return -ENOMEM;
	/*
	 * Save the se_tmr_req for qla_tgt_xmit_tm_rsp() callback into LLD code
	 */
	mcmd->se_tmr_req = se_cmd->se_tmr_req;
	/*
	 * Locate the underlying TCM struct se_lun from sc->device->lun
	 */
	if (transport_get_lun_for_tmr(se_cmd, lun) < 0) {
		transport_generic_free_cmd(se_cmd, 1, 1, 0);
		return -EINVAL;
	}
	/*
	 * Queue the TMR associated se_cmd into TCM Core for processing
	 */
	return transport_generic_handle_tmr(se_cmd);
}

int tcm_qla2xxx_queue_data_in(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	cmd->bufflen = se_cmd->data_length;
	cmd->dma_data_direction = se_cmd->data_direction;
	cmd->aborted = atomic_read(&T_TASK(se_cmd)->t_transport_aborted);
	/*
	 * Setup the struct se_task->task_sg[] chained SG list
	 */
	if ((se_cmd->se_cmd_flags & SCF_SCSI_DATA_SG_IO_CDB) ||
	    (se_cmd->se_cmd_flags & SCF_SCSI_CONTROL_SG_IO_CDB)) {
		transport_do_task_sg_chain(se_cmd);

		cmd->sg_cnt = T_TASK(se_cmd)->t_tasks_sg_chained_no;
		cmd->sg = T_TASK(se_cmd)->t_tasks_sg_chained;
	} else if (se_cmd->se_cmd_flags & SCF_SCSI_CONTROL_NONSG_IO_CDB) {
		/*
		 * Use T_TASK(se_cmd)->t_tasks_sg_bounce for control CDBs
		 * using a contigious buffer
		 */
		sg_init_table(&T_TASK(se_cmd)->t_tasks_sg_bounce, 1);
		sg_set_buf(&T_TASK(se_cmd)->t_tasks_sg_bounce,
			T_TASK(se_cmd)->t_task_buf, se_cmd->data_length);

		cmd->sg_cnt = 1;
		cmd->sg = &T_TASK(se_cmd)->t_tasks_sg_bounce;
	} else {
		cmd->sg_cnt = 0;
		cmd->sg = NULL;
	}

	cmd->offset = 0;

	/*
	 * Now queue completed DATA_IN the qla2xxx LLD and response ring
	 */
	return qla2xxx_xmit_response(cmd, QLA_TGT_XMIT_DATA|QLA_TGT_XMIT_STATUS,
				se_cmd->scsi_status);
}

int tcm_qla2xxx_queue_status(struct se_cmd *se_cmd)
{
	struct qla_tgt_cmd *cmd = container_of(se_cmd, struct qla_tgt_cmd, se_cmd);

	cmd->bufflen = se_cmd->data_length;
	cmd->sg = NULL;
	cmd->sg_cnt = 0;
	cmd->offset = 0;
	cmd->dma_data_direction = se_cmd->data_direction;
	cmd->aborted = atomic_read(&T_TASK(se_cmd)->t_transport_aborted);

	/*
	 * Now queue status response to qla2xxx LLD code and response ring
	 */
	return qla2xxx_xmit_response(cmd, QLA_TGT_XMIT_STATUS, se_cmd->scsi_status);
}

int tcm_qla2xxx_queue_tm_rsp(struct se_cmd *se_cmd)
{
	struct se_tmr_req *se_tmr = se_cmd->se_tmr_req;
	struct qla_tgt_mgmt_cmd *mcmd = container_of(se_cmd,
				struct qla_tgt_mgmt_cmd, se_cmd);

	printk("queue_tm_rsp: mcmd: %p func: 0x%02x response: 0x%02x\n",
			mcmd, se_tmr->function, se_tmr->response);
	/*
	 * Do translation between TCM TM response codes and
	 * QLA2xxx FC TM response codes.
	 */
	switch (se_tmr->response) {
	case TMR_FUNCTION_COMPLETE:
		mcmd->fc_tm_rsp = FC_TM_SUCCESS;
		break;
	case TMR_TASK_DOES_NOT_EXIST:
		mcmd->fc_tm_rsp = FC_TM_BAD_CMD;
		break;
	case TMR_FUNCTION_REJECTED:
		mcmd->fc_tm_rsp = FC_TM_REJECT;
		break;
	case TMR_LUN_DOES_NOT_EXIST:
	default:
		mcmd->fc_tm_rsp = FC_TM_FAILED;
		break;
	}
	/*
	 * Queue the TM response to QLA2xxx LLD to build a
	 * CTIO response packet.
	 */
	qla_tgt_xmit_tm_rsp(mcmd);

	return 0;
}

u16 tcm_qla2xxx_get_fabric_sense_len(void)
{
	return 0;
}

u16 tcm_qla2xxx_set_fabric_sense_len(struct se_cmd *se_cmd, u32 sense_length)
{
	return 0;
}

int tcm_qla2xxx_is_state_remove(struct se_cmd *se_cmd)
{
	return 0;
}

u64 tcm_qla2xxx_pack_lun(unsigned int lun)
{
	WARN_ON(lun >= 256);
	/* Caller wants this byte-swapped */
	return cpu_to_le64((lun & 0xff) << 8);
}
